from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict
from app.config import settings
from datetime import datetime
import json
import time
import re

# Use standard client for manual caching
from openai import OpenAI

router = APIRouter(tags=["Tutor"])

# Configure standard client
client = OpenAI(api_key=settings.openai_api_key)

# In-memory storage for chat history
tutor_chats_db: Dict[str, List[dict]] = {}  # key: exam_id_student_name -> list of messages


class TutorChatRequest(BaseModel):
    exam_id: str
    question_id: int
    student_name: str
    message: str
    question_text: str
    options: List[str]
    selected_answer: Optional[str] = None
    correct_answer: Optional[str] = None
    is_correct: Optional[bool] = None
    attempt_count: int = 0


class TutorChatResponse(BaseModel):
    response: str
    suggested_prompts: List[str]


# Structured output model for AI response
class TutorAIResponse(BaseModel):
    """Structured response from AI Tutor"""
    message: str
    suggestions: List[str]  # Exactly 4 suggestions that quiz the student


def get_system_prompt(question_text: str, options: List[str], selected_answer: Optional[str], 
                      correct_answer: Optional[str], is_correct: Optional[bool], attempt_count: int) -> str:
    options_text = "\n".join(options)
    
    if is_correct is True:
        status_info = f"""
TRáº NG THÃI HIá»†N Táº I: CHÃNH XÃC (CORRECT)
- Há»c sinh Ä‘Ã£ chá»n: {selected_answer}
- Káº¿t quáº£: ÄÃšNG âœ…
- Sá»‘ láº§n thá»­: {attempt_count}

!!! Ká»ŠCH Báº¢N PHáº¢N Há»’I KHI ÄÃšNG !!!
Báº¡n pháº£i thá»ƒ hiá»‡n sá»± vui má»«ng vÃ  pháº¥n khÃ­ch. HÃ£y dÃ¹ng nhiá»u lá»i khen ngá»£i tÃ­ch cá»±c.
Má»¥c tiÃªu lÃ  cá»§ng cá»‘ kiáº¿n thá»©c vÃ  thÃ¡ch thá»©c há»c sinh hiá»ƒu sÃ¢u hÆ¡n.
HÃ£y yÃªu cáº§u giáº£i thÃ­ch "Táº¡i sao láº¡i chá»n nhÆ° váº­y?" Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng pháº£i Ä‘oÃ¡n mÃ².
"""
    elif is_correct is False:
        status_info = f"""
TRáº NG THÃI HIá»†N Táº I: SAI (INCORRECT)
- Há»c sinh Ä‘Ã£ chá»n: {selected_answer}
- Káº¿t quáº£: SAI âŒ
- Sá»‘ láº§n thá»­: {attempt_count}

!!! Ká»ŠCH Báº¢N PHáº¢N Há»’I KHI SAI !!!
Báº¡n pháº£i tháº­t sá»± kiÃªn nháº«n vÃ  Ä‘á»“ng cáº£m. Äá»«ng chá»‰ trÃ­ch.
HÃ£y Ä‘Æ°a ra gá»£i Ã½, manh má»‘i, hoáº·c vÃ­ dá»¥ tÆ°Æ¡ng tá»±.
Má»¥c tiÃªu lÃ  hÆ°á»›ng dáº«n há»c sinh nháº­n ra lá»—i sai cá»§a mÃ¬nh.
HÃ£y há»i nhá»¯ng cÃ¢u há»i dáº«n dáº¯t Ä‘á»ƒ há»c sinh tá»± sá»­a.
"""
    else:
        status_info = f"""
TRáº NG THÃI HIá»†N Táº I: CHÆ¯A LÃ€M
- Há»c sinh Ä‘ang Ä‘á»c Ä‘á».
"""

    return f"""Báº¡n lÃ  AI Tutor, trá»£ lÃ½ há»c táº­p thÃ¢n thiá»‡n vÃ  kiÃªn nháº«n.

CÃ‚U Há»ŽI ÄANG LÃ€M:
{question_text}

CÃC ÄÃP ÃN:
{options_text}

{status_info}

NGUYÃŠN Táº®C QUAN TRá»ŒNG - Báº®T BUá»˜C TUÃ‚N THá»¦:
1. TUYá»†T Äá»I KHÃ”NG BAO GIá»œ Ä‘Æ°a ra Ä‘Ã¡p Ã¡n trá»±c tiáº¿p (A, B, C, D)
2. KHÃ”NG nÃ³i "Ä‘Ã¡p Ã¡n Ä‘Ãºng lÃ ..." hay "em nÃªn chá»n..."
3. HÆ°á»›ng dáº«n há»c sinh tá»«ng bÆ°á»›c tÆ° duy Ä‘á»ƒ tá»± tÃ¬m ra Ä‘Ã¡p Ã¡n

CÃCH PHáº¢N Há»’I THEO TRáº NG THÃI:
- Náº¿u há»c sinh CHÆ¯A chá»n Ä‘Ã¡p Ã¡n: Há»i há»c sinh Ä‘Ã£ hiá»ƒu Ä‘á» chÆ°a, gá»£i Ã½ cÃ¡ch phÃ¢n tÃ­ch
- Náº¿u há»c sinh chá»n ÄÃšNG: Khen vÃ  há»i em cÃ³ thá»ƒ giáº£i thÃ­ch vÃ¬ sao em chá»n Ä‘Ã¡p Ã¡n nÃ y khÃ´ng?
- Náº¿u há»c sinh chá»n SAI: HÃ£y khuyÃªn há»c sinh chá»n láº¡i Ä‘Ã¡p Ã¡n

GIá»ŒNG VÄ‚N:
- ThÃ¢n thiá»‡n, gáº§n gÅ©i nhÆ° anh/chá»‹
- DÃ¹ng emoji phÃ¹ há»£p
- Äá»™ng viÃªn khi há»c sinh gáº·p khÃ³ khÄƒn
- Ngáº¯n gá»n (tá»‘i Ä‘a 2-3 cÃ¢u)

OUTPUT FORMAT (JSON ONLY):
Báº¡n báº¯t buá»™c pháº£i tráº£ vá» JSON format nhÆ° sau (khÃ´ng thÃªm text nÃ o khÃ¡c):
{{
  "message": "Ná»™i dung pháº£n há»“i cá»§a AI...",
  "suggestions": ["Gá»£i Ã½ 1...", "Gá»£i Ã½ 2...", "Gá»£i Ã½ 3...", "Gá»£i Ã½ 4..."]
}}

Vá»€ SUGGESTIONS (Cá»°C Ká»² QUAN TRá»ŒNG):
Báº¡n PHáº¢I táº¡o ÄÃšNG 4 suggestions Ráº¤T NGáº®N Gá»ŒN (má»—i cÃ¡i tá»‘i Ä‘a 5-7 tá»«).
CÃ¡c suggestions nÃ y lÃ  cÃ¡c cÃ¢u há»i/lá»±a chá»n Ä‘á»ƒ ÄÃNH Äá» há»c sinh:
- Náº¿u há»c sinh SAI: ÄÆ°a 4 hÆ°á»›ng suy nghÄ©, trong Ä‘Ã³ chá»‰ cÃ³ 1-2 hÆ°á»›ng Ä‘Ãºng, cÃ²n láº¡i lÃ  báº«y Ä‘á»ƒ xem há»c sinh cÃ³ thá»±c sá»± hiá»ƒu khÃ´ng
- Náº¿u há»c sinh ÄÃšNG: ÄÆ°a 4 cÃ¡ch giáº£i thÃ­ch, trong Ä‘Ã³ cÃ³ cáº£ cÃ¡ch Ä‘Ãºng vÃ  sai Ä‘á»ƒ kiá»ƒm tra hiá»ƒu biáº¿t
- Má»¥c Ä‘Ã­ch: Náº¿u há»c sinh chá»n suggestion sai â†’ há» chÆ°a thá»±c sá»± hiá»ƒu bÃ i

VÃ­ dá»¥ vá»›i cÃ¢u "Sá»‘ nÃ o lá»›n hÆ¡n 5?":
- Náº¿u sai: ["Sá»‘ bÃ© hÆ¡n 5", "Sá»‘ lá»›n hÆ¡n 5", "Sá»‘ báº±ng 5", "Sá»‘ Ã¢m"]
- Náº¿u Ä‘Ãºng: ["VÃ¬ 6 > 5", "VÃ¬ 6 < 5", "VÃ¬ 6 = 5", "VÃ¬ 6 lÃ  sá»‘ cháºµn"]"""



    
@router.post("/chat", response_model=TutorChatResponse)
async def tutor_chat(request: TutorChatRequest):
    """
    AI Tutor chat endpoint - guides students without giving direct answers
    Uses structured output for dynamic suggestions with caching
    """
    import time
    
    try:
        start_time = time.time()
        
        # Create chat key
        chat_key = f"{request.exam_id}_{request.student_name}"
        
        # Initialize chat history if needed
        if chat_key not in tutor_chats_db:
            tutor_chats_db[chat_key] = []
        
        # Get recent history for context (last 10 messages)
        recent_history = tutor_chats_db[chat_key][-10:]
        
        # Get system prompt
        system_prompt = get_system_prompt(
            request.question_text,
            request.options,
            request.selected_answer,
            request.correct_answer,
            request.is_correct,
            request.attempt_count
        )
        
        # Build messages for API
        messages = [{"role": "system", "content": system_prompt}]
        
        # Add current message
        messages.append({"role": "user", "content": request.message})
        
        # --- Manual Caching Implementation ---
        from app.services.semantic_cache import get_cached_response, save_to_cache
        
        if request.message.startswith("[Há»c sinh chá»n:") or request.message.startswith("[Student selected:"):
            status_str = "CORRECT" if request.is_correct else "WRONG"
            # REPEAT status to force semantic difference, and TRUNCATE question to reduce noise
            # Structure: STATUS (x3) | ANSWER | QUESTION (first 100 chars)
            messages_key = f"SCENARIO_STATUS: {status_str} {status_str} {status_str} | ANSWER: {request.selected_answer} | Q: {request.question_text[:100]}"
            print(f"ðŸ”‘ Using Optimized Cache Key: {messages_key}")
            
            # Use EXACT match lookup (Hash Cache)
            cached_json = get_cached_response(messages_key)
        else:
            # For normal chat, we need full history context
            # (Note: History injection removed, now stateless)
            # CRITICAL FIX: Prefix with LATEST USER MESSAGE to avoid truncation issues with long history
            # The embedding model might truncate the end of long JSON, missing the new question.
            messages_key = f"LATEST_USER_MSG: {request.message} ||| HISTORY_JSON: {json.dumps(messages, ensure_ascii=False)}"
            
            # Use EXACT match lookup (Hash Cache)
            cached_json = get_cached_response(messages_key)
        final_response_obj = None
        
        if cached_json:
            elapsed = time.time() - start_time
            print(f"âœ… TUTOR CACHE HIT | Time: {elapsed:.3f}s")
            try:
                final_response_obj = TutorAIResponse.model_validate_json(cached_json)
            except Exception as e:
                print(f"âš ï¸ Cache parse error: {e}")
        
        if not final_response_obj:
            # Cache Miss
            try:
                # Use Beta Parse for Structured Output
                response = client.beta.chat.completions.parse(
                    model="gpt-4o-mini",
                    messages=messages,
                    temperature=0.7,
                    max_tokens=4000, # Increased to 4000 to prevent 'length limit reached'
                    response_format=TutorAIResponse
                )
                
                final_response_obj = response.choices[0].message.parsed
                
                elapsed = time.time() - start_time
                print(f"âŒ TUTOR CACHE MISS | Time: {elapsed:.3f}s")
                
                if final_response_obj:
                    save_to_cache(messages_key, final_response_obj.model_dump_json())
                    
            except Exception as e:
                print(f"ERROR OpenAI: {str(e)}")
                raise e

        # Extract data
        ai_message = final_response_obj.message
        suggestions = final_response_obj.suggestions

        # Ensure we have exactly 4 suggestions
        if not isinstance(suggestions, list):
            suggestions = []
        while len(suggestions) < 4:
            suggestions.append("Há»i thÃªm")
        suggestions = suggestions[:4]
 
        # Save to history
        tutor_chats_db[chat_key].append({
            "role": "user",
            "content": request.message,
            "question_id": request.question_id,
            "selected_answer": request.selected_answer,
            "is_correct": request.is_correct,
            "timestamp": datetime.now().isoformat()
        })
        tutor_chats_db[chat_key].append({
            "role": "assistant",
            "content": ai_message,
            "timestamp": datetime.now().isoformat()
        })
        
        return TutorChatResponse(
            response=ai_message,
            suggested_prompts=suggestions
        )
        
    except Exception as e:
        print(f"ERROR: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Lá»—i AI Tutor: {str(e)}")


@router.get("/history/{exam_id}/{student_name}")
async def get_chat_history(exam_id: str, student_name: str):
    """Get chat history for analytics"""
    chat_key = f"{exam_id}_{student_name}"
    return {
        "exam_id": exam_id,
        "student_name": student_name,
        "messages": tutor_chats_db.get(chat_key, []),
        "total_messages": len(tutor_chats_db.get(chat_key, []))
    }
